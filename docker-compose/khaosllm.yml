# docker-compose/khaosllm.yml
# KhaosLLM - Sovereign AI Inference Stack
# Air-gapped local LLM deployment with Ollama + Qdrant

version: '3.8'

services:
  # Ollama - Local LLM inference engine
  ollama:
    image: ollama/ollama:latest
    container_name: khaosllm-ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"  # Localhost only for security
    volumes:
      - ollama-models:/root/.ollama
      - ./ollama-config:/config:ro
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=/root/.ollama/models
      # GPU support - uncomment for NVIDIA
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - khaosnet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    labels:
      - "com.strategickhaos.service=khaosllm"
      - "com.strategickhaos.tier=ai"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Qdrant - Vector database for RAG and memory
  qdrant:
    image: qdrant/qdrant:latest
    container_name: khaosllm-qdrant
    restart: unless-stopped
    ports:
      - "127.0.0.1:6333:6333"  # REST API
      - "127.0.0.1:6334:6334"  # gRPC
    volumes:
      - qdrant-storage:/qdrant/storage
      - ./qdrant-config:/qdrant/config:ro
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    networks:
      - khaosnet
    labels:
      - "com.strategickhaos.service=khaosllm-vector"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis - Session cache and rate limiting
  redis:
    image: redis:7-alpine
    container_name: khaosllm-redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server
      --save 60 1
      --loglevel warning
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    networks:
      - khaosnet
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    labels:
      - "com.strategickhaos.service=khaosllm-cache"

  # LiteLLM Proxy - Unified API gateway (optional)
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: khaosllm-proxy
    restart: unless-stopped
    ports:
      - "127.0.0.1:4000:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_CONFIG_PATH=/app/config.yaml
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - khaosnet
    depends_on:
      - ollama
    labels:
      - "com.strategickhaos.service=khaosllm-proxy"

  # Model initialization - runs once to pull models
  ollama-init:
    image: ollama/ollama:latest
    container_name: khaosllm-init
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    networks:
      - khaosnet
    depends_on:
      - ollama
    entrypoint: /bin/sh
    command: |
      -c "
      sleep 10 &&
      ollama pull qwen2.5:72b &&
      ollama pull llama3.2:70b &&
      ollama pull codellama:34b &&
      ollama pull mistral:7b &&
      echo 'Models initialized successfully'
      "
    restart: "no"
    labels:
      - "com.strategickhaos.service=khaosllm-init"

volumes:
  ollama-models:
    driver: local
  qdrant-storage:
    driver: local
  redis-data:
    driver: local

networks:
  khaosnet:
    driver: bridge
    name: khaosnet
    external: true
