# docker-compose/khaosllm.yml
# KhaosLLM - Sovereign AI Inference Stack
# Air-gapped local LLM deployment with Ollama + Qdrant

version: '3.8'

services:
  # Ollama - Local LLM inference engine
  ollama:
    image: ollama/ollama:latest
    container_name: khaosllm-ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"  # Localhost only for security
    volumes:
      - ollama-models:/root/.ollama
      - ./ollama-config:/config:ro
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=/root/.ollama/models
      # GPU support - uncomment for NVIDIA
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - khaosnet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    labels:
      - "com.strategickhaos.service=khaosllm"
      - "com.strategickhaos.tier=ai"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Qdrant - Vector database for RAG and memory
  qdrant:
    image: qdrant/qdrant:latest
    container_name: khaosllm-qdrant
    restart: unless-stopped
    ports:
      - "127.0.0.1:6333:6333"  # REST API
      - "127.0.0.1:6334:6334"  # gRPC
    volumes:
      - qdrant-storage:/qdrant/storage
      - ./qdrant-config:/qdrant/config:ro
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    networks:
      - khaosnet
    labels:
      - "com.strategickhaos.service=khaosllm-vector"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis - Session cache and rate limiting
  redis:
    image: redis:7-alpine
    container_name: khaosllm-redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server
      --save 60 1
      --loglevel warning
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    networks:
      - khaosnet
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    labels:
      - "com.strategickhaos.service=khaosllm-cache"

  # LiteLLM Proxy - Unified API gateway (optional)
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: khaosllm-proxy
    restart: unless-stopped
    ports:
      - "127.0.0.1:4000:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_CONFIG_PATH=/app/config.yaml
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - khaosnet
    depends_on:
      - ollama
    labels:
      - "com.strategickhaos.service=khaosllm-proxy"

  # Model initialization - runs once to pull models
  # NOTE: This init container is disabled by default to avoid automatic large downloads
  # To manually pull models after deployment, run:
  #   docker exec khaosllm-ollama ollama pull mistral:7b
  #   docker exec khaosllm-ollama ollama pull qwen2.5:72b
  #   docker exec khaosllm-ollama ollama pull llama3.2:70b
  # 
  # Uncomment below to enable automatic model pulling on first start
  # ollama-init:
  #   image: ollama/ollama:latest
  #   container_name: khaosllm-init
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=ollama:11434
  #   networks:
  #     - khaosnet
  #   depends_on:
  #     - ollama
  #   entrypoint: /bin/sh
  #   command: |
  #     -c "
  #     echo 'Waiting for Ollama to be ready...' &&
  #     sleep 15 &&
  #     echo 'Pulling mistral:7b (~4GB)...' &&
  #     (ollama pull mistral:7b || echo 'Failed to pull mistral:7b') &&
  #     echo 'Pulling qwen2.5:72b (~40GB)...' &&
  #     (ollama pull qwen2.5:72b || echo 'Failed to pull qwen2.5:72b') &&
  #     echo 'Pulling llama3.2:70b (~38GB)...' &&
  #     (ollama pull llama3.2:70b || echo 'Failed to pull llama3.2:70b') &&
  #     echo 'Model initialization complete. Check status with: docker exec khaosllm-ollama ollama list'
  #     "
  #   restart: "no"
  #   labels:
  #     - "com.strategickhaos.service=khaosllm-init"

volumes:
  ollama-models:
    driver: local
  qdrant-storage:
    driver: local
  redis-data:
    driver: local

networks:
  khaosnet:
    driver: bridge
    name: khaosnet
    external: true
